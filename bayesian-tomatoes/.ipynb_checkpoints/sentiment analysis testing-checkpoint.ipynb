{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "-- css --\n",
      "\n",
      "<style>\n",
      "div.text_cell_render {\n",
      "    line-height: 150%;\n",
      "    font-size: 110%;\n",
      "    width: 800px;\n",
      "    margin-left:50px;\n",
      "    margin-right:auto;\n",
      "    }\n",
      "    \n",
      "div.Andrew {\n",
      "    margin: 20px;\n",
      "    padding: 20px;\n",
      "    color: maroon;\n",
      "    background-color: rgb(233,233,233);\n",
      "    font-family: calibri;\n",
      "    font-size: 14pt;\n",
      "}\n",
      "</style>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "<div class=Andrew>\n",
      "    What: Development playground for CS109 final project\n",
      "    <br /><br />\n",
      "Who: Andrew Reece, Daniel Wu, Javier Pineda, Baris Baloglu\n",
      "    <br /><br />\n",
      "Uses: code snippets from HW2, HW3\n",
      "</div>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Not sure what we'll need exactly, so just importing all the usual suspects here\n",
      "\n",
      "import json\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import re\n",
      "import scipy.stats as stats\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "pd.set_option('display.width', 500)\n",
      "pd.set_option('display.max_columns', 30)\n",
      "\n",
      "# set some nicer defaults for matplotlib\n",
      "from matplotlib import rcParams\n",
      "\n",
      "#these colors come from colorbrewer2.org. Each is an RGB triplet\n",
      "dark2_colors = [(0.10588235294117647, 0.6196078431372549, 0.4666666666666667),\n",
      "                (0.8509803921568627, 0.37254901960784315, 0.00784313725490196),\n",
      "                (0.4588235294117647, 0.4392156862745098, 0.7019607843137254),\n",
      "                (0.9058823529411765, 0.1607843137254902, 0.5411764705882353),\n",
      "                (0.4, 0.6509803921568628, 0.11764705882352941),\n",
      "                (0.9019607843137255, 0.6705882352941176, 0.00784313725490196),\n",
      "                (0.6509803921568628, 0.4627450980392157, 0.11372549019607843),\n",
      "                (0.4, 0.4, 0.4)]\n",
      "\n",
      "rcParams['figure.figsize'] = (10, 6)\n",
      "rcParams['figure.dpi'] = 150\n",
      "rcParams['axes.color_cycle'] = dark2_colors\n",
      "rcParams['lines.linewidth'] = 2\n",
      "rcParams['axes.grid'] = False\n",
      "rcParams['axes.facecolor'] = 'white'\n",
      "rcParams['font.size'] = 14\n",
      "rcParams['patch.edgecolor'] = 'none'\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div class=\"Andrew\">\n",
      "For now we're using the NYT developer API. \n",
      "<br /><br />\n",
      "It doesn't give full article text, but we can get headline and lead paragraph.\n",
      "<br /><br />\n",
      "The code below only uses the main \"Article\" API.  \n",
      "<br />\n",
      "But I've posted API keys for other types of APIs offered by NYT.\n",
      "</div>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "## NYT API keys\n",
      "api_key_article = \"851e7d0a131bee9bc01097470c238637:13:47475506\"\n",
      "api_key_community = \"519167db119ee6408c4ee51b3c391e11:0:47475506\"\n",
      "api_key_geo = \"a984ad78bf017f0ade1fcd980aa6353f:15:47475506\"\n",
      "api_key_popular = \"09dfaf288ad6c2ec46893a27ca758d41:19:47475506\"\n",
      "api_key_movies = \"e8a48f7d7731698b05267146c681c352:5:47475506\"\n",
      "api_key_semantic = \"9063b41607bbf486247b8e596a1456b8:7:47475506\"\n",
      "api_key_newswire = \"209ebb7b0ab44094970e8b39c63fea7e:2:47475506\"\n",
      "api_key_timestags = \"43b3366f288db10cb019fd532299723f:10:47475506\"\n",
      "\n",
      "## data gets tuples of (whole api return, lead paragraphs)\n",
      "data = []\n",
      "\n",
      "## just picked a few terms meant to have a fair spread in content\n",
      "terms = ['mindfulness', 'debt', 'kardashian']\n",
      "\n",
      "## loop through terms\n",
      "for term in terms:\n",
      "    \n",
      "    ## api request for each term\n",
      "    url = ''.join([\"http://api.nytimes.com/svc/search/v2/articlesearch.json?q=\",term,\"&api-key=\",api_key_article])\n",
      "    req = requests.get(url).text\n",
      "    \n",
      "    ## decode into json dicts\n",
      "    jsons = json.loads(req)\n",
      "    \n",
      "   \n",
      "    ## collects lead paragraphs\n",
      "    leads = []\n",
      "    \n",
      "    ## loop through each article returned from API request\n",
      "    for doc in jsons['response']['docs']:\n",
      "        \n",
      "        ## sometimes we have weird ASCII encodings with text, so we decode to utf-8\n",
      "        try:\n",
      "            if doc['lead_paragraph'].encode('utf-8') != 'None':\n",
      "            \n",
      "                leads.append(doc['lead_paragraph'].encode('utf-8'))\n",
      "        \n",
      "        ## however, sometimes we get NoneTypes returned, which barf on .encode() method\n",
      "        except AttributeError:\n",
      "        \n",
      "            if str(doc['lead_paragraph']) != 'None':\n",
      "            \n",
      "                leads.append(str(doc['lead_paragraph']))\n",
      "\n",
      "    \n",
      "    ## append to data[]\n",
      "    data.append((json,leads))\n",
      "    \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 49
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div class=\"Andrew\">\n",
      "Below is an example of the structure of our list data[]"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## structure of data[]\n",
      "\n",
      "## eg. \n",
      "## third search term ('Kardashian', index 2)\n",
      "## lead paragraphs (second tuple entry, index 1)\n",
      "## first paragraph (index 0)\n",
      "\n",
      "print data[0][1][0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "A term for mental training reaches the height of trendiness, and like yoga before it, may be leaving its mark.\n"
       ]
      }
     ],
     "prompt_number": 63
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div class=\"Andrew\">\n",
      "Now we'll pull in sentiment analysis data.  \n",
      "<br />This is the database being used at <a href=\"http://onehappybird.com\">onehappybird.com.\n",
      "</a></div>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## read sentiment data from text file\n",
      "sentiment = pd.read_table('sentiment_data.txt', sep='\\t')\n",
      "\n",
      "## have a look\n",
      "print sentiment.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "        word  happiness_rank  happiness_average  happiness_standard_deviation  twitter_rank  google_rank  nyt_rank  lyrics_rank\n",
        "0   laughter               1               8.50                        0.9313          3600          NaN       NaN         1728\n",
        "1  happiness               2               8.44                        0.9723          1853         2458       NaN         1230\n",
        "2       love               3               8.42                        1.1082            25          317       328           23\n",
        "3      happy               4               8.30                        0.9949            65         1372      1313          375\n",
        "4    laughed               5               8.26                        1.1572          3334         3542       NaN         2332\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "//anaconda/python.app/Contents/lib/python2.7/site-packages/pandas/core/config.py:570: DeprecationWarning: height has been deprecated.\n",
        "\n",
        "  warnings.warn(d.msg, DeprecationWarning)\n"
       ]
      }
     ],
     "prompt_number": 54
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div class=\"Andrew\">\n",
      "Eventually we'll want to run the analysis on all lead paragraphs, across all search terms. \n",
      "<br /><br />\n",
      "For now, we'll narrow it down to paragraphs from just one search term, to keep it simple.\n",
      "</div>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## for now, set 'leads' variable as lead paragraphs for second search term ('debt')\n",
      "leads = data[1][1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 67
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## these three lists are our bins for collecting data on each lead paragraph\n",
      "avg_word = []\n",
      "post_length = []\n",
      "avg_sent = []\n",
      "\n",
      "## loop through lead paragraphs\n",
      "for item in leads:\n",
      "    \n",
      "    ## bins for local (ie. un-aggregated) sentiment scores and character lengths per word\n",
      "    sent_scores = []\n",
      "    word_lengths = []\n",
      "    \n",
      "    ## keep track of word count for adding to post_length[]\n",
      "    wdct = 0\n",
      "    \n",
      "    ## loop through each word - get word list by splitting on whitespace\n",
      "    for word in item.split(' '):\n",
      "        \n",
      "        ## remove non-alphabet characters \n",
      "        word = re.sub(\"\\W\",'',word)\n",
      "        \n",
      "        ## only continue with actual words (vs empty strings)\n",
      "        if len(word) > 0:\n",
      "\n",
      "            ## increment word count\n",
      "            wdct += 1\n",
      "            \n",
      "            ## add word length to bin\n",
      "            word_lengths.append(len(word))\n",
      "            \n",
      "            ## loop through words in sentiment df\n",
      "            for idx, entry in sentiment.iterrows():\n",
      "\n",
      "                ## if our word in question is found in sentiment df\n",
      "                ## then store its happiness rating in sent_scores\n",
      "                if word == entry['word']:\n",
      "                        \n",
      "                    sent_scores.append(entry['happiness_average'])\n",
      "    \n",
      "    ## post length is the word count\n",
      "    post_length.append(wdct)    \n",
      "    \n",
      "    ## take averages of word lengths and sentiment scores, add to bins\n",
      "    avg_word.append(round(np.mean(word_lengths),2))\n",
      "    avg_sent.append(round(np.mean(sent_scores),3))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 68
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div class=\"Andrew\">\n",
      "Pandas dataframes make it easy to collect and reference the vectors we're creating.\n",
      "</div>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## make a pandas df out of these vectors for easy reference/manipulation\n",
      "post_df = pd.DataFrame(data={'avg word length':avg_word, 'post length':post_length, 'avg sentiment':avg_sent})\n",
      "\n",
      "## some posts may get entered when technically they're not posts...just 0-length strings.  so exclude those.\n",
      "## we can probably get rid of these earlier on...\n",
      "trimmed_post_df = post_df[post_df['post length'] > 1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 69
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div class=\"Andrew\">\n",
      "We can do simple Pearson's correlation statistics to see whether there's any relationship between our variables of interest.\n",
      "</div>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## pearsonr() gives a tuple of (Pearson zero-order correlation, p-value)\n",
      "\n",
      "## here we print out the correlation matrix for avg word length, post length, and avg sentiment\n",
      "## (not very pretty though - better to output in a readable table, a la SPSS)\n",
      "\n",
      "print 'avg word length : post length'\n",
      "print stats.pearsonr(trimmed_post_df['avg word length'].values, trimmed_post_df['post length'].values)\n",
      "\n",
      "print 'avg word length : avg sentiment'\n",
      "print stats.pearsonr(trimmed_post_df['avg word length'].values, trimmed_post_df['avg sentiment'].values)\n",
      "\n",
      "print 'post length : avg sentiment'\n",
      "print stats.pearsonr(trimmed_post_df['post length'].values, trimmed_post_df['avg sentiment'].values)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div class=\"Andrew\">It looks like word length and sentiment is trending toward a significant positive correlation, and possibly post length and avg sentiment heading towards a negative.  \n",
      "<br />\n",
      "    It's a really small sample compared to our entire corpus, so best to retain some skepticism...next is to run on a dozen or so major search terms and look at the aggregate correlations.</div>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Everything below is pasted from HW3...it's where we want to go next.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text = leads\n",
      "print \"Original text is\\n\", '\\n'.join(text)\n",
      "\n",
      "vectorizer = CountVectorizer(min_df=0)\n",
      "\n",
      "# call `fit` to build the vocabulary\n",
      "vectorizer.fit(text)\n",
      "\n",
      "# call `transform` to convert text to a bag of words\n",
      "x = vectorizer.transform(text)\n",
      "\n",
      "# CountVectorizer uses a sparse array to save memory, but it's easier in this assignment to \n",
      "# convert back to a \"normal\" numpy array\n",
      "x = x.toarray()\n",
      "\n",
      "print\n",
      "print \"Transformed text vector is \\n\", x\n",
      "\n",
      "# `get_feature_names` tracks which word is associated with each column of the transformed x\n",
      "print\n",
      "print \"Words for each feature:\"\n",
      "print vectorizer.get_feature_names()\n",
      "\n",
      "# Notice that the bag of words treatment doesn't preserve information about the *order* of words, \n",
      "# just their frequency"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Original text is\n",
        "A term for mental training reaches the height of trendiness, and like yoga before it, may be leaving its mark.\n",
        "The other night at a dinner party, a friend described how she tried to practice mindfulness meditation to keep herself from losing it during an utterly wretched seven-hour layover in an airport while she was exhausted, ill and desperate to get home to her children.\n",
        "Mindfulness is terrific for the person practicing it, but what about for her friends and family?\n",
        "Peter Williams sits cross-legged on an upholstered chair near mine, eyes closed, hands resting near his socks. A thick, starchy silence pools around us. Williams takes several slow breaths and then begins, narrating aloud the contents of his mind: ''. . . hearing . . . enjoying . . . buzzing in my feet . . . fear: will my feet be O.K.? . . . hearing . . . breathing . . . wondering: will my parents read this article? . . . enjoying . . . afraid . . . what do you think of me? . . . fear . . . it's O.K. . . . compassion . . . butterflies in belly . . . fear . . . kindness . . . compassion . . . hearing . . . bird, house finch . . . naming . . . thinking . . . joy. . . .''\n",
        "None\n",
        "Everybody has to multitask at work. But when doctors multitask rather than focus on just practicing medicine, does the doctor-patient relationship suffer?\n",
        "One night during my training, long after all the other doctors had fled the hospital, I found a senior surgeon still on the wards working on a patient note. He was a surgeon with extraordinary skill, a doctor of few words whose folksy quips had become the stuff of department legend. ''I'm sorry you're still stuck here,'' I said, walking up to him. He looked up from the chart. ''I'm not working tomorrow, so I'm just fine.''\n",
        "One night during my training, long after all the other doctors had fled the hospital, I found a senior surgeon still on the wards working on a patient note. He was a surgeon with extraordinary skill, a doctor of few words whose folksy quips had become the stuff of department legend. ''I'm sorry you're still stuck here,'' I said, walking up to him. He looked up from the chart. ''I'm not working tomorrow, so I'm just fine.''\n",
        "Misconceptions surround the practice of mindfulness, which is part meditation and part a greater awareness of the things around you.\n",
        "In this week&#39;s Doctor and Patient column, Dr. Pauline Chen writes about an ever-widening sea of distractions, including paperwork, insurance battles and electronic records and communications, that can interfere with a doctor&#39;s focus on medicine. And she explores a solution -- meditation and mindfulness to help the doctor get back to the business of doctoring.\n",
        "\n",
        "Transformed text vector is \n",
        "[[0 0 0 ..., 0 1 0]\n",
        " [0 0 0 ..., 0 0 0]\n",
        " [0 1 0 ..., 0 0 0]\n",
        " ..., \n",
        " [0 0 0 ..., 0 0 1]\n",
        " [0 0 0 ..., 0 0 1]\n",
        " [2 1 0 ..., 1 0 0]]\n",
        "\n",
        "Words for each feature:\n",
        "[u'39', u'about', u'afraid', u'after', u'airport', u'all', u'aloud', u'an', u'and', u'around', u'article', u'at', u'awareness', u'back', u'battles', u'be', u'become', u'before', u'begins', u'belly', u'bird', u'breathing', u'breaths', u'business', u'but', u'butterflies', u'buzzing', u'can', u'chair', u'chart', u'chen', u'children', u'closed', u'column', u'communications', u'compassion', u'contents', u'cross', u'department', u'described', u'desperate', u'dinner', u'distractions', u'do', u'doctor', u'doctoring', u'doctors', u'does', u'dr', u'during', u'electronic', u'enjoying', u'ever', u'everybody', u'exhausted', u'explores', u'extraordinary', u'eyes', u'family', u'fear', u'feet', u'few', u'finch', u'fine', u'fled', u'focus', u'folksy', u'for', u'found', u'friend', u'friends', u'from', u'get', u'greater', u'had', u'hands', u'has', u'he', u'hearing', u'height', u'help', u'her', u'here', u'herself', u'him', u'his', u'home', u'hospital', u'hour', u'house', u'how', u'ill', u'in', u'including', u'insurance', u'interfere', u'is', u'it', u'its', u'joy', u'just', u'keep', u'kindness', u'layover', u'leaving', u'legend', u'legged', u'like', u'long', u'looked', u'losing', u'mark', u'may', u'me', u'medicine', u'meditation', u'mental', u'mind', u'mindfulness', u'mine', u'misconceptions', u'multitask', u'my', u'naming', u'narrating', u'near', u'night', u'none', u'not', u'note', u'of', u'on', u'one', u'other', u'paperwork', u'parents', u'part', u'party', u'patient', u'pauline', u'person', u'peter', u'pools', u'practice', u'practicing', u'quips', u'rather', u're', u'reaches', u'read', u'records', u'relationship', u'resting', u'said', u'sea', u'senior', u'seven', u'several', u'she', u'silence', u'sits', u'skill', u'slow', u'so', u'socks', u'solution', u'sorry', u'starchy', u'still', u'stuck', u'stuff', u'suffer', u'surgeon', u'surround', u'takes', u'term', u'terrific', u'than', u'that', u'the', u'then', u'thick', u'things', u'think', u'thinking', u'this', u'to', u'tomorrow', u'training', u'trendiness', u'tried', u'up', u'upholstered', u'us', u'utterly', u'walking', u'wards', u'was', u'week', u'what', u'when', u'which', u'while', u'whose', u'widening', u'will', u'williams', u'with', u'wondering', u'words', u'work', u'working', u'wretched', u'writes', u'yoga', u'you']\n"
       ]
      }
     ],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def make_xy(pa, vectorizer=None):\n",
      "    #Your code here    \n",
      "\n",
      "    X = []\n",
      "    Y = []\n",
      "    \n",
      "    # if no vectorizer is passed to function, create one with min_df=0\n",
      "    if vectorizer == None:\n",
      "        \n",
      "        vectorizer = CountVectorizer(min_df=0)\n",
      "    \n",
      "    # fit vectorizer to entire quote repertoire\n",
      "    vectorizer.fit(critics.quote)\n",
      "    # make bag of words from quotes\n",
      "    X = vectorizer.transform(critics.quote)\n",
      "    \n",
      "    # convert fresh/rotten (strings) to 1/0 ints\n",
      "    for idx, q in enumerate(critics['quote']):\n",
      "        \n",
      "        fresh = critics['fresh'].irow(idx)\n",
      "        if fresh == 'fresh':\n",
      "            val = 1\n",
      "        elif fresh == 'rotten':\n",
      "            val = 0\n",
      "        Y.append(val)\n",
      "    \n",
      "    # return X/Y\n",
      "    # X can stay as a scarce array for now, Y covert from list to numpy array\n",
      "    return X,np.asarray(Y)\n",
      "\n",
      "X, Y = make_xy(critics)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}