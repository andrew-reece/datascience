{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Not sure what we'll need exactly, so just importing all the usual suspects here\n",
      "%matplotlib inline\n",
      "\n",
      "from bs4 import BeautifulSoup\n",
      "import json\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import re\n",
      "import scipy.stats as stats\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn import linear_model\n",
      "import nltk\n",
      "from nltk.corpus import stopwords #If this line doesn't work, use nltk.downloads() and download corpus#\n",
      "from collections import defaultdict \n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "# set some nicer defaults for matplotlib\n",
      "from matplotlib import rcParams\n",
      "\n",
      "#these colors come from colorbrewer2.org. Each is an RGB triplet\n",
      "dark2_colors = [(0.10588235294117647, 0.6196078431372549, 0.4666666666666667),\n",
      "                (0.8509803921568627, 0.37254901960784315, 0.00784313725490196),\n",
      "                (0.4588235294117647, 0.4392156862745098, 0.7019607843137254),\n",
      "                (0.9058823529411765, 0.1607843137254902, 0.5411764705882353),\n",
      "                (0.4, 0.6509803921568628, 0.11764705882352941),\n",
      "                (0.9019607843137255, 0.6705882352941176, 0.00784313725490196),\n",
      "                (0.6509803921568628, 0.4627450980392157, 0.11372549019607843),\n",
      "                (0.4, 0.4, 0.4)]\n",
      "\n",
      "rcParams['figure.figsize'] = (10, 6)\n",
      "rcParams['figure.dpi'] = 150\n",
      "rcParams['axes.color_cycle'] = dark2_colors\n",
      "rcParams['lines.linewidth'] = 2\n",
      "rcParams['axes.grid'] = False\n",
      "rcParams['axes.facecolor'] = 'white'\n",
      "rcParams['font.size'] = 14\n",
      "rcParams['patch.edgecolor'] = 'none'\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "section_list_url = ''.join([\"http://api.nytimes.com/svc/mostpopular/v2/mostshared/sections-list.json?api-key=\",\n",
      "                            api_key_popular])\n",
      "\n",
      "sections = requests.get(section_list_url).text\n",
      "for item in json.loads(sections)[\"results\"]:\n",
      "    print item[\"name\"]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Arts\n",
        "Automobiles\n",
        "Books\n",
        "Booming\n",
        "Business Day\n",
        "Crosswords & Games\n",
        "Dining & Wine\n",
        "Education\n",
        "Fashion & Style\n",
        "Giving\n",
        "Great Homes and Destinations\n",
        "Health\n",
        "Home & Garden\n",
        "Job Market\n",
        "Magazine\n",
        "Movies\n",
        "Multimedia\n",
        "N.Y. / Region\n",
        "Opinion\n",
        "Public Editor\n",
        "Real Estate\n",
        "Science\n",
        "Sports\n",
        "Style\n",
        "Sunday Review\n",
        "T Magazine\n",
        "T:Style\n",
        "Technology\n",
        "Theater\n",
        "Travel\n",
        "U.S.\n",
        "World\n",
        "Your Money\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## NYT API keys\n",
      "api_key_article = \"851e7d0a131bee9bc01097470c238637:13:47475506\"\n",
      "api_key_community = \"519167db119ee6408c4ee51b3c391e11:0:47475506\"\n",
      "api_key_geo = \"a984ad78bf017f0ade1fcd980aa6353f:15:47475506\"\n",
      "api_key_popular = \"09dfaf288ad6c2ec46893a27ca758d41:19:47475506\"\n",
      "api_key_movies = \"e8a48f7d7731698b05267146c681c352:5:47475506\"\n",
      "api_key_semantic = \"9063b41607bbf486247b8e596a1456b8:7:47475506\"\n",
      "api_key_newswire = \"209ebb7b0ab44094970e8b39c63fea7e:2:47475506\"\n",
      "api_key_timestags = \"43b3366f288db10cb019fd532299723f:10:47475506\"\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## read sentiment data from text file\n",
      "sentiment = pd.read_table('sentiment_data.txt', sep='\\t')\n",
      "\n",
      "## remove stopwords\n",
      "nltk.download()\n",
      "StopWordList = stopwords.words(\"english\")\n",
      "sentiment['in_stopword'] = False\n",
      "sentiment['in_stopword'][sentiment.word.isin(StopWordList)] = True\n",
      "sentiment = sentiment[sentiment.in_stopword == False]\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "NLTK Downloader\n",
        "---------------------------------------------------------------------------\n",
        "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
        "---------------------------------------------------------------------------\n"
       ]
      },
      {
       "name": "stdout",
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Downloader> u\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Nothing to update.\n",
        "\n",
        "---------------------------------------------------------------------------\n",
        "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
        "---------------------------------------------------------------------------\n"
       ]
      },
      {
       "name": "stdout",
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Downloader> d\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Download which package (l=list; x=cancel)?\n"
       ]
      },
      {
       "name": "stdout",
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  Identifier> l\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Packages:\n",
        "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
        "  [ ] abc................. Australian Broadcasting Commission 2006\n",
        "  [ ] alpino.............. Alpino Dutch Treebank\n",
        "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
        "                           Extraction Systems in Biology)\n",
        "  [ ] brown............... Brown Corpus\n",
        "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
        "  [ ] cess_cat............ CESS-CAT Treebank\n",
        "  [ ] cess_esp............ CESS-ESP Treebank\n",
        "  [ ] chat80.............. Chat-80 Data Files\n",
        "  [ ] city_database....... City Database\n",
        "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
        "  [ ] comtrans............ ComTrans Corpus Sample\n",
        "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
        "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
        "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
        "                           and Basque Subset)\n",
        "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
        "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
        "                           Corpus\n"
       ]
      },
      {
       "name": "stdout",
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Hit Enter to continue: \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  [ ] floresta............ Portuguese Treebank\n",
        "  [ ] framenet_v15........ FrameNet 1.5\n",
        "  [ ] gazetteers.......... Gazeteer Lists\n",
        "  [ ] genesis............. Genesis Corpus\n",
        "  [ ] gutenberg........... Project Gutenberg Selections\n",
        "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
        "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
        "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
        "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
        "                           ChaSen format)\n",
        "  [ ] kimmo............... PC-KIMMO Data Files\n",
        "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
        "  [ ] langid.............. Language Id Corpus\n",
        "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
        "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
        "                           part-of-speech tags\n",
        "  [ ] machado............. Machado de Assis -- Obra Completa\n",
        "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
        "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
        "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
        "  [ ] nps_chat............ NPS Chat\n"
       ]
      },
      {
       "name": "stdout",
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Hit Enter to continue: \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  [ ] oanc_masc........... Open American National Corpus: Manually\n",
        "                           Annotated Sub-Corpus\n",
        "  [ ] paradigms........... Paradigm Corpus\n",
        "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
        "                           Evaluation Shared Task\n",
        "  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n",
        "  [ ] pl196x.............. Polish language of the XX century sixties\n",
        "  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n",
        "  [ ] problem_reports..... Problem Report Corpus\n",
        "  [ ] propbank............ Proposition Bank Corpus 1.0\n",
        "  [ ] ptb................. Penn Treebank\n",
        "  [ ] qc.................. Experimental Data for Question Classification\n",
        "  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
        "                           version\n",
        "  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
        "  [ ] semcor.............. SemCor 3.0\n",
        "  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
        "  [ ] shakespeare......... Shakespeare XML Corpus Sample\n",
        "  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n",
        "  [ ] smultron............ SMULTRON Corpus Sample\n",
        "  [ ] state_union......... C-Span State of the Union Address Corpus\n"
       ]
      },
      {
       "name": "stdout",
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Hit Enter to continue: \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  [ ] stopwords........... Stopwords Corpus\n",
        "  [ ] swadesh............. Swadesh Wordlists\n",
        "  [ ] switchboard......... Switchboard Corpus Sample\n",
        "  [ ] timit............... TIMIT Corpus Sample\n",
        "  [ ] toolbox............. Toolbox Sample Files\n",
        "  [ ] treebank............ Penn Treebank Sample\n",
        "  [ ] udhr................ Universal Declaration of Human Rights Corpus\n",
        "  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n",
        "                           (Unicode Version)\n",
        "  [ ] unicode_samples..... Unicode Samples\n",
        "  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n",
        "  [ ] webtext............. Web Text Corpus\n",
        "  [ ] wordnet............. WordNet\n",
        "  [ ] wordnet_ic.......... WordNet-InfoContent\n",
        "  [ ] words............... Word Lists\n",
        "  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n",
        "                           English Prose\n",
        "  [ ] basque_grammars..... Grammars for Basque\n",
        "  [ ] book_grammars....... Grammars from NLTK Book\n",
        "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
        "                           for parser comparison\n"
       ]
      },
      {
       "name": "stdout",
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Hit Enter to continue: \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  [ ] sample_grammars..... Sample Grammars\n",
        "  [ ] spanish_grammars.... Grammars for Spanish\n",
        "  [ ] tagsets............. Help on Tagsets\n",
        "  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
        "                           Portuguesa)\n",
        "  [ ] hmm_treebank_pos_tagger Treebank Part of Speech Tagger (HMM)\n",
        "  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n",
        "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
        "  [ ] punkt............... Punkt Tokenizer Models\n",
        "\n",
        "Collections:\n",
        "  [ ] all-corpora......... All the corpora\n",
        "  [ ] all................. All packages\n",
        "  [ ] book................ Everything used in the NLTK Book\n",
        "\n",
        "([*] marks installed packages)\n",
        "\n",
        "Download which package (l=list; x=cancel)?\n"
       ]
      },
      {
       "name": "stdout",
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  Identifier> all-corpora\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "    Downloading collection 'all-corpora'\n",
        "       | \n",
        "       | Downloading package 'abc' to /Users/andrew/nltk_data...\n",
        "       |   Unzipping corpora/abc.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       | Downloading package 'alpino' to /Users/andrew/nltk_data..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       |   Unzipping corpora/alpino.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       | Downloading package 'biocreative_ppi' to\n",
        "       |     /Users/andrew/nltk_data..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       |   Unzipping corpora/biocreative_ppi.zip."
       ]
      }
     ],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print sentiment.head()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "        word  happiness_rank  happiness_average  happiness_standard_deviation  \\\n",
        "0   laughter               1               8.50                        0.9313   \n",
        "1  happiness               2               8.44                        0.9723   \n",
        "2       love               3               8.42                        1.1082   \n",
        "3      happy               4               8.30                        0.9949   \n",
        "4    laughed               5               8.26                        1.1572   \n",
        "\n",
        "   twitter_rank  google_rank  nyt_rank  lyrics_rank  \n",
        "0          3600          NaN       NaN         1728  \n",
        "1          1853         2458       NaN         1230  \n",
        "2            25          317       328           23  \n",
        "3            65         1372      1313          375  \n",
        "4          3334         3542       NaN         2332  \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "//anaconda/python.app/Contents/lib/python2.7/site-packages/pandas/core/config.py:570: DeprecationWarning: height has been deprecated.\n",
        "\n",
        "  warnings.warn(d.msg, DeprecationWarning)\n",
        "//anaconda/python.app/Contents/lib/python2.7/site-packages/pandas/core/config.py:570: DeprecationWarning: height has been deprecated.\n",
        "\n",
        "  warnings.warn(d.msg, DeprecationWarning)\n"
       ]
      }
     ],
     "prompt_number": 194
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## NYT: MOST POPULAR\n",
      "## here, we grab the first 100 most shared articles from the past 30 days in NYT\n",
      "\n",
      "popular_urls = []\n",
      "calls = []\n",
      "\n",
      "## only 20 articles returned at a time\n",
      "## offset specifies where in the full list to start from\n",
      "## eg. offset = 20 gets articles 21-40\n",
      "\n",
      "for offset in range(0,100,20):\n",
      "    \n",
      "    url = ''.join([\"http://api.nytimes.com/svc/mostpopular/v2/mostviewed/all-sections/30.json?api-key=\",\n",
      "                    api_key_popular,\n",
      "                    \"&offset=\",\n",
      "                    str(offset)])\n",
      "    \n",
      "    popular_urls.append(url)\n",
      "    \n",
      "    ## get raw html for each article\n",
      "    ## via NYT Most Popular API\n",
      "    ## http://developer.nytimes.com/docs/most_popular_api/\n",
      "    \n",
      "    calls.append(requests.get(url).text)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 189
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "ct = 0\n",
      "for call in calls:\n",
      "    ct +=1\n",
      "    jsons = json.loads(call)\n",
      "    if ct < 5:\n",
      "        for j in jsons[\"results\"]:\n",
      "            print j.keys()\n",
      "            print 'views:',j[\"views\"]\n",
      "            print 'des facet:',j[\"des_facet\"]\n",
      "            print 'section:',j[\"section\"]\n",
      "            print 'column:',j[\"column\"]\n",
      "            #print 'total shares',j[\"total_shares\"]\n",
      "            print '\\n\\n'\n",
      "            '''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 192,
       "text": [
        "'\\nct = 0\\nfor call in calls:\\n    ct +=1\\n    jsons = json.loads(call)\\n    if ct < 5:\\n        for j in jsons[\"results\"]:\\n            print j.keys()\\n            print \\'views:\\',j[\"views\"]\\n            print \\'des facet:\\',j[\"des_facet\"]\\n            print \\'section:\\',j[\"section\"]\\n            print \\'column:\\',j[\"column\"]\\n            #print \\'total shares\\',j[\"total_shares\"]\\n            print \\'\\n\\n\\'\\n            '"
       ]
      }
     ],
     "prompt_number": 192
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def make_soup(j):\n",
      "    \n",
      "    raw_article_html = requests.get(j[\"url\"]).text\n",
      "        \n",
      "    soup = BeautifulSoup(raw_article_html)\n",
      "    \n",
      "    return soup\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 195
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_article_text(soup):\n",
      "            \n",
      "    ## after looking through the actual article page source code\n",
      "    ## we can see that this 'itemprop' attribute stores all blocks of article text\n",
      "    ## so we parse out only these blocks from the whole text dump (there are lots of ads, images, menus, etc)\n",
      "        \n",
      "    text = soup.findAll(itemprop=\"articleBody\")\n",
      "    \n",
      "    return text\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 241
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def cleanup_text(text):\n",
      "    \n",
      "    ## apostrophes (') get turned into a funny French a\n",
      "    ## so we sub back in ' with regex\n",
      "    ## we also get rid of carriage returns and other useless whitespace \n",
      "    \n",
      "    remove_a = re.compile(u'\u00e2')\n",
      "    remove_w = re.compile('[\\n\\r\\t]')  \n",
      "    \n",
      "    full_text = [ remove_a.sub(ur\"\", t.contents[0]) for t in text if len(t.contents) > 0 ]\n",
      "    full_text = [ remove_w.sub(r\"\", ft) for ft in full_text ]\n",
      "        \n",
      "    ## dump entire article into a single variable\n",
      "        \n",
      "    cleaned_text = ' '.join(full_text)     \n",
      "    \n",
      "    return cleaned_text\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 248
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_sentiment_scores(df):\n",
      "    \n",
      "    article_happiness = stat.nanmean(df.happiness_average)\n",
      "    \n",
      "    print 'article_happiness:',article_happiness\n",
      "    \n",
      "    return article_happiness"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def text_to_words(article):\n",
      "    \n",
      "    words_df2 = pd.DataFrame(article.split(' '), columns=['word']) \n",
      "\n",
      "## split articles into collections of individual words\n",
      "\n",
      "    #words_df2 = words_df.copy()\n",
      "    words_df2 = words_df2.drop_duplicates()\n",
      "    words_df2['word_length'] = words_df2.word.apply(lambda x: len(x))\n",
      "    words_df2['word'] = words_df2.word.apply(lambda x: re.sub(\"\\W\", '', x))\n",
      "    words_df2 = words_df2[words_df2.word_length > 2]\n",
      "        \n",
      "    senti_wordbag_df = pd.merge(sentiment, words_df2, how=\"right\", on=\"word\")\n",
      "\n",
      "    article_happiness = get_sentiment_scores(senti_wordbag_df)\n",
      "    \n",
      "    word_count = words_df2.shape[0]\n",
      "    \n",
      "    return word_count, senti_wordbag_df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 258
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## this code block tests out all our helper functions which we use in the big block below\n",
      "\n",
      "test_call = calls[0]\n",
      "\n",
      "test_json = json.loads(test_call)[\"results\"][0]\n",
      "## print test_json\n",
      "\n",
      "test_soup = make_soup(test_json)\n",
      "## print test_soup\n",
      "\n",
      "test_text = get_article_text(soup)\n",
      "## print test_text\n",
      "\n",
      "test_cleantext = cleanup_text(test_text)\n",
      "## print test_cleantext\n",
      "\n",
      "test_count, test_senti_df = text_to_words(test_cleantext)\n",
      "## print 'count:',test_count\n",
      "## print 'df:',test_senti_df.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 7.8   7.72  7.38  7.26  7.22  7.22  7.18  7.1   7.02  6.92  6.82  6.8\n",
        "  6.76  6.74  6.7   6.66  6.64  6.64  6.6   6.58  6.56  6.56  6.52  6.52\n",
        "  6.5   6.46  6.44  6.44  6.42  6.4   6.4   6.38  6.34  6.34  6.32  6.3\n",
        "  6.3   6.3   6.28  6.28  6.28  6.28  6.26  6.24  6.22  6.2   6.2   6.16\n",
        "  6.16  6.1   6.06  6.04  6.02  6.    6.    5.96  5.94  5.92  5.9   5.9\n",
        "  5.9   5.9   5.88  5.88  5.88  5.86  5.84  5.82  5.82  5.82  5.8   5.8\n",
        "  5.78  5.78  5.76  5.76  5.74  5.72  5.72  5.72  5.68  5.68  5.68  5.68\n",
        "  5.66  5.66  5.64  5.64  5.64  5.62  5.62  5.62  5.62  5.62  5.6   5.58\n",
        "  5.58  5.56  5.56  5.56  5.55  5.54  5.54  5.54  5.54  5.54  5.54  5.54\n",
        "  5.54  5.52  5.5   5.48  5.48  5.48  5.46  5.46  5.42  5.42  5.42  5.4\n",
        "  5.4   5.38  5.38  5.38  5.38  5.36  5.36  5.34  5.34  5.34  5.34  5.3\n",
        "  5.3   5.3   5.28  5.28  5.28  5.28  5.28  5.26  5.26  5.24  5.22  5.22\n",
        "  5.22  5.2   5.18  5.18  5.18  5.16  5.16  5.16  5.16  5.16  5.16  5.16\n",
        "  5.14  5.14  5.12  5.12  5.1   5.1   5.1   5.1   5.08  5.06  5.06  5.06\n",
        "  5.04  5.04  5.04  5.04  5.02  5.    5.    5.    4.98  4.96  4.94  4.94\n",
        "  4.94  4.94  4.94  4.94  4.92  4.92  4.92  4.9   4.9   4.88  4.86  4.84\n",
        "  4.82  4.76  4.74  4.74  4.74  4.74  4.72  4.7   4.6   4.5   4.5   4.48\n",
        "  4.38  4.36  4.24  4.16  4.14  3.86  3.8   3.78  3.74  3.72  3.66  3.66\n",
        "  3.66  3.66  3.66  3.38  3.32  3.1   2.98  2.92  2.76  2.7   2.64  2.24\n",
        "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
        "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
        "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
        "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
        "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
        "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
        "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
        "   nan   nan]\n",
        "5.43188596491\n"
       ]
      }
     ],
     "prompt_number": 259
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## extract article body text from raw html\n",
      "\n",
      "## the API return only gives us metadata and lead paragraph\n",
      "## but we do get the URL\n",
      "## so here we load each article URL and scrape the text\n",
      "## then we store the actual article text bodies in a list\n",
      "\n",
      "articles = []\n",
      "\n",
      "## STEPS IN THIS CODE BLOCK:\n",
      "##\n",
      "## 1) loop through each API call\n",
      "## 2) process output into json\n",
      "## 3) grab metadata from json\n",
      "## 4) scrape article text from url\n",
      "## 5) clean article text\n",
      "## 6) store in master list of articles\n",
      "\n",
      "    #1\n",
      "for call in calls:\n",
      "    \n",
      "    jsons = json.loads(call)\n",
      "    \n",
      "    #2\n",
      "    \n",
      "    for j in jsons[\"results\"]:\n",
      "        \n",
      "    #3\n",
      "        title = j[\"title\"]  \n",
      "        section = j[\"section\"]\n",
      "        \n",
      "    #4\n",
      "        soup = make_soup(j)\n",
      "    \n",
      "    #5\n",
      "        text = get_article_text(soup)\n",
      "        \n",
      "    ## perform regex on weird or useless characters\n",
      "    ## some nodes fail on the regex so we do try/except\n",
      "        \n",
      "        try:\n",
      "            \n",
      "            cleaned_text = cleanup_text(text)\n",
      "            \n",
      "        except: \n",
      "            \n",
      "            print 'error with regex on full_text!'\n",
      "            continue\n",
      "        \n",
      "    ## create df of individual words in the article\n",
      "    ## compute article length\n",
      "    \n",
      "        article_length, sentiword_df = text_to_words(cleaned_text)\n",
      "    \n",
      "    ## append article text to master list 'articles'\n",
      "        \n",
      "    #6\n",
      "        articles.append([title, section, article_length, full_text])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 216
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "most_popular_df = pd.DataFrame(index = range(0,len(articles)), columns = [\"title\",\"section\",\"shares\",\"text\"])\n",
      "\n",
      "for idx, a in enumerate(articles):\n",
      "    #print a[0]\n",
      "    most_popular_df.ix[idx, \"title\"] = a[0]\n",
      "    most_popular_df.ix[idx, \"section\"] = a[1]\n",
      "    most_popular_df.ix[idx, \"shares\"] = a[2]\n",
      "    most_popular_df.ix[idx, \"text\"] = a[3]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 175
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "most_popular_df.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 183,
       "text": [
        "(99, 4)"
       ]
      }
     ],
     "prompt_number": 183
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "most_popular_by_shares = most_popular_df.sort(columns=\"shares\", ascending=False)\n",
      "print most_popular_by_shares.head(20)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "                                                title        section shares  \\\n",
        "58           Rake the Leaves? Some Towns Say Mow Them  N.Y. / Region     20   \n",
        "38            Learning How to Die in the Anthropocene        Opinion     20   \n",
        "98  Night Falls, and 5Pointz, a Graffiti Mecca, Is...  N.Y. / Region     20   \n",
        "18    Baffling 400,000-Year-Old Clue to Human Origins        Science     20   \n",
        "78                Honoring Female Pioneers in Science        Science     20   \n",
        "37                                 My Upper Peninsula         Travel     19   \n",
        "17     Risk Calculator for Cholesterol Appears Flawed         Health     19   \n",
        "97  Oxytocin Found to Stimulate Social Brain Regio...         Health     19   \n",
        "77     Cut in Food Stamps Forces Hard Choices on Poor           U.S.     19   \n",
        "57                  The Shame of American Health Care        Opinion     19   \n",
        "96  For Son of a Nazi-Era Dealer, a Private Life A...          World     18   \n",
        "56                                 A Permanent Slump?        Opinion     18   \n",
        "16                   Don\u2019t Give More Patients Statins        Opinion     18   \n",
        "76         Caught in a Revolving Door of Unemployment   Business Day     18   \n",
        "36  Young and Educated in Europe, but Desperate fo...          World     18   \n",
        "95                          36 Hours in Kauai, Hawaii         Travel     17   \n",
        "35                               Millennial Searchers        Opinion     17   \n",
        "55                                    Sex as Exercise         Health     17   \n",
        "75                      Why the Dutch Love Black Pete        Opinion     17   \n",
        "15  Mandela\u2019s Death Leaves South Africa Without It...          World     17   \n",
        "\n",
        "                                                 text  \n",
        "58  DOBBS FERRY, N.Y. \u0080\u0094 They have been burned, bl...  \n",
        "38                                                     \n",
        "98  Graffiti is often denounced as vandalism, but ...  \n",
        "18  Scientists have found the oldest DNA evidence ...  \n",
        "78                                                     \n",
        "37  About 10 years ago, before we sold our farm in...  \n",
        "17  Last week, the nation\u0080\u0099s leading heart organiz...  \n",
        "97  The hormone oxytocin has been generating excit...  \n",
        "77  CHARLESTON, S.C. \u0080\u0094 For many, a $10 or $20 cut...  \n",
        "57  Even as Americans struggle with the changes re...  \n",
        "96  MUNICH \u0080\u0094 As an expert in works of art that th...  \n",
        "56  Spend any time around monetary officials and o...  \n",
        "16  ON Tuesday, the  This announcement is not a re...  \n",
        "76  On a cold October morning, just after the fede...  \n",
        "36  MADRID \u0080\u0094 Alba M\u00c3\u00a9ndez, a 24-year-old with a m...  \n",
        "95  These days, Kauai sits firmly on the tourist r...  \n",
        "35  FOR Viktor Frankl, the Holocaust survivor who ...  \n",
        "55                                                     \n",
        "75  WHEN I was growing up in Amsterdam in the 1970...  \n",
        "15  JOHANNESBURG \u0080\u0094 Nelson Mandela, South Africa\u0080\u0099...  \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "//anaconda/python.app/Contents/lib/python2.7/site-packages/pandas/core/config.py:570: DeprecationWarning: height has been deprecated.\n",
        "\n",
        "  warnings.warn(d.msg, DeprecationWarning)\n",
        "//anaconda/python.app/Contents/lib/python2.7/site-packages/pandas/core/config.py:570: DeprecationWarning: height has been deprecated.\n",
        "\n",
        "  warnings.warn(d.msg, DeprecationWarning)\n"
       ]
      }
     ],
     "prompt_number": 181
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## article search\n",
      "\n",
      "url = ''.join([\"http://api.nytimes.com/svc/search/v2/articlesearch.json?q=\", term,\n",
      "                       \"&begin_date=\", begindate, \n",
      "                       \"&end_date=\", enddate,               \n",
      "                       \"&api-key=\",api_key_article])\n",
      "                       \n",
      "req = requests.get(url).text\n",
      "        \n",
      "        ## decode into json dicts\n",
      "jsons = json.loads(req)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}