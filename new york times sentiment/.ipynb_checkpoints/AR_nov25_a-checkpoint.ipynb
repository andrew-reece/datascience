{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "-- css --\n",
      "\n",
      "<style>\n",
      "div.text_cell_render {\n",
      "    line-height: 150%;\n",
      "    font-size: 110%;\n",
      "    width: 800px;\n",
      "    margin-left:50px;\n",
      "    margin-right:auto;\n",
      "    }\n",
      "    \n",
      "div.Andrew {\n",
      "    margin: 20px;\n",
      "    padding: 20px;\n",
      "    color: maroon;\n",
      "    background-color: rgb(233,233,233);\n",
      "    font-family: calibri;\n",
      "    font-size: 14pt;\n",
      "}\n",
      "</style>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "<div class=Andrew>\n",
      "    What: Development playground for CS109 final project\n",
      "    <br /><br />\n",
      "Who: Andrew Reece, Daniel Wu, Javier Pineda, Baris Baloglu\n",
      "    <br /><br />\n",
      "Uses: code snippets from HW2, HW3\n",
      "</div>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Not sure what we'll need exactly, so just importing all the usual suspects here\n",
      "\n",
      "import json\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import re\n",
      "import scipy.stats as stats\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn import linear_model\n",
      "\n",
      "pd.set_option('display.width', 500)\n",
      "pd.set_option('display.max_columns', 30)\n",
      "\n",
      "# set some nicer defaults for matplotlib\n",
      "from matplotlib import rcParams\n",
      "\n",
      "#these colors come from colorbrewer2.org. Each is an RGB triplet\n",
      "dark2_colors = [(0.10588235294117647, 0.6196078431372549, 0.4666666666666667),\n",
      "                (0.8509803921568627, 0.37254901960784315, 0.00784313725490196),\n",
      "                (0.4588235294117647, 0.4392156862745098, 0.7019607843137254),\n",
      "                (0.9058823529411765, 0.1607843137254902, 0.5411764705882353),\n",
      "                (0.4, 0.6509803921568628, 0.11764705882352941),\n",
      "                (0.9019607843137255, 0.6705882352941176, 0.00784313725490196),\n",
      "                (0.6509803921568628, 0.4627450980392157, 0.11372549019607843),\n",
      "                (0.4, 0.4, 0.4)]\n",
      "\n",
      "rcParams['figure.figsize'] = (10, 6)\n",
      "rcParams['figure.dpi'] = 150\n",
      "rcParams['axes.color_cycle'] = dark2_colors\n",
      "rcParams['lines.linewidth'] = 2\n",
      "rcParams['axes.grid'] = False\n",
      "rcParams['axes.facecolor'] = 'white'\n",
      "rcParams['font.size'] = 14\n",
      "rcParams['patch.edgecolor'] = 'none'\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 54
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div class=\"Andrew\">\n",
      "For now we're using the NYT developer API. \n",
      "<br /><br />\n",
      "It doesn't give full article text, but we can get headline and lead paragraph.\n",
      "<br /><br />\n",
      "The code below only uses the main \"Article\" API.  \n",
      "<br />\n",
      "But I've posted API keys for other types of APIs offered by NYT.\n",
      "</div>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "## NYT API keys\n",
      "api_key_article = \"851e7d0a131bee9bc01097470c238637:13:47475506\"\n",
      "api_key_community = \"519167db119ee6408c4ee51b3c391e11:0:47475506\"\n",
      "api_key_geo = \"a984ad78bf017f0ade1fcd980aa6353f:15:47475506\"\n",
      "api_key_popular = \"09dfaf288ad6c2ec46893a27ca758d41:19:47475506\"\n",
      "api_key_movies = \"e8a48f7d7731698b05267146c681c352:5:47475506\"\n",
      "api_key_semantic = \"9063b41607bbf486247b8e596a1456b8:7:47475506\"\n",
      "api_key_newswire = \"209ebb7b0ab44094970e8b39c63fea7e:2:47475506\"\n",
      "api_key_timestags = \"43b3366f288db10cb019fd532299723f:10:47475506\"\n",
      "\n",
      "\n",
      "begindate = \"20000101\" #YYYYMMDD\n",
      "enddate = \"20131112\" #YYYYMMDD\n",
      "\n",
      "\n",
      "## just picked a few terms meant to have a fair spread in content\n",
      "terms = ['mindfulness', 'debt', 'kardashian']\n",
      "\n",
      "# intially an empty dataframe\n",
      "data_df = pd.DataFrame()\n",
      "\n",
      "## loop through terms\n",
      "for term in terms:\n",
      "    \n",
      "    ## api request for each term\n",
      "    url = ''.join([\"http://api.nytimes.com/svc/search/v2/articlesearch.json?q=\", term,\n",
      "                   \"&begin_date=\", begindate, \n",
      "                   \"&end_date=\", enddate,\n",
      "                   \"&api-key=\",api_key_article])\n",
      "                   \n",
      "    req = requests.get(url).text\n",
      "    \n",
      "    ## decode into json dicts\n",
      "    jsons = json.loads(req)\n",
      "    \n",
      "    ## loop through each article returned from API request\n",
      "    for doc in jsons['response']['docs']:\n",
      "\n",
      "        # making dataframe; a term will appear in multiple rows\n",
      "        doc_df = pd.DataFrame([term], columns = ['term'])\n",
      "        \n",
      "        # alternative way of dealing with none-types\n",
      "        if json.dumps(doc['abstract']) != 'null':\n",
      "            \n",
      "            # encode weird ascii stuff\n",
      "            abstract = [doc['abstract'].encode('utf-8')]\n",
      "            \n",
      "            # making dataframe and adding abstracts\n",
      "            doc_df['abstract'] = abstract\n",
      "            \n",
      "        # this way we take all data, and we can do away with what we don't want later    \n",
      "        else:\n",
      "            abstract = np.nan\n",
      "            doc_df['abstract'] = abstract\n",
      "            \n",
      "        # can add an if-else clause for anything you want to get, then add to doc_df\n",
      "        if json.dumps(doc['lead_paragraph']) != 'null':\n",
      "            \n",
      "            lead = [doc['lead_paragraph'].encode('utf-8')]\n",
      "            doc_df['lead'] = lead\n",
      "            \n",
      "        else:\n",
      "            lead = np.nan\n",
      "            doc_df['lead'] = lead\n",
      "        \n",
      "        data_df = pd.concat([data_df, doc_df]).reset_index(drop=True)\n",
      "        \n",
      "leads = list(data_df.lead)\n",
      "#print leads\n",
      "data_df\n",
      "        \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>abstract</th>\n",
        "      <th>lead</th>\n",
        "      <th>term</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0 </th>\n",
        "      <td>                                               NaN</td>\n",
        "      <td> A term for mental training reaches the height ...</td>\n",
        "      <td> mindfulness</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1 </th>\n",
        "      <td> Mindfulness is terrific for the person practic...</td>\n",
        "      <td> The other night at a dinner party, a friend de...</td>\n",
        "      <td> mindfulness</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2 </th>\n",
        "      <td>                                               NaN</td>\n",
        "      <td> Mindfulness is terrific for the person practic...</td>\n",
        "      <td> mindfulness</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3 </th>\n",
        "      <td> Jennifer Egan article on her experience at Spi...</td>\n",
        "      <td> Peter Williams sits cross-legged on an upholst...</td>\n",
        "      <td> mindfulness</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4 </th>\n",
        "      <td> Two-thirds of doctors experience the emotional...</td>\n",
        "      <td>                                               NaN</td>\n",
        "      <td> mindfulness</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5 </th>\n",
        "      <td> In this week's Doctor and Patient column, Dr. ...</td>\n",
        "      <td> Everybody has to multitask at work. But when d...</td>\n",
        "      <td> mindfulness</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6 </th>\n",
        "      <td>                                               NaN</td>\n",
        "      <td> One night during my training, long after all t...</td>\n",
        "      <td> mindfulness</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>7 </th>\n",
        "      <td> Dr Michael S Krasner, University of Rochester ...</td>\n",
        "      <td> One night during my training, long after all t...</td>\n",
        "      <td> mindfulness</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>8 </th>\n",
        "      <td> Alina Tugend Shortcuts column observes that mi...</td>\n",
        "      <td> Misconceptions surround the practice of mindfu...</td>\n",
        "      <td> mindfulness</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>9 </th>\n",
        "      <td>                                               NaN</td>\n",
        "      <td> In this week&amp;#39;s Doctor and Patient column, ...</td>\n",
        "      <td> mindfulness</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>10</th>\n",
        "      <td> Paul Krugman Op-Ed column contends decision by...</td>\n",
        "      <td> It\u2019s another case of ideology posing as econom...</td>\n",
        "      <td>        debt</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>11</th>\n",
        "      <td> Frank Bruni Op-Ed column expresses alarm at th...</td>\n",
        "      <td> So much treasure, so much waste. Italy is a ca...</td>\n",
        "      <td>        debt</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>12</th>\n",
        "      <td>                                               NaN</td>\n",
        "      <td> In 2008, a year when the global financial syst...</td>\n",
        "      <td>        debt</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>13</th>\n",
        "      <td> Paul Krugman Op-Ed column Germany's angry reac...</td>\n",
        "      <td> They are beggaring their neighbors, and the wo...</td>\n",
        "      <td>        debt</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>14</th>\n",
        "      <td>                                               NaN</td>\n",
        "      <td> ''The Debt,'' John Madden's remake of a 2007 I...</td>\n",
        "      <td>        debt</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>15</th>\n",
        "      <td> Jason Anthony and Karl Cluck discuss their boo...</td>\n",
        "      <td> Been there, spent that. It took two Columbia g...</td>\n",
        "      <td>        debt</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>16</th>\n",
        "      <td> Some student loan rates doubled as of July 1, ...</td>\n",
        "      <td>                                               NaN</td>\n",
        "      <td>        debt</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>17</th>\n",
        "      <td>                                               NaN</td>\n",
        "      <td> What we have is what we have. We have to rely ...</td>\n",
        "      <td>        debt</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>18</th>\n",
        "      <td> Stephen Holden reviews documentary Life and De...</td>\n",
        "      <td> The term ''globalization'' is so tinged with r...</td>\n",
        "      <td>        debt</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>19</th>\n",
        "      <td> Paul Krugman Op-Ed column warns policy makers ...</td>\n",
        "      <td> The long-term costs of short-run failure are p...</td>\n",
        "      <td>        debt</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>20</th>\n",
        "      <td> Kim Kardashian, sexy reality television star a...</td>\n",
        "      <td> FOR many longtime readers of W, the elite soci...</td>\n",
        "      <td>  kardashian</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>21</th>\n",
        "      <td> Kim Kardashian discusses growing up in Beverly...</td>\n",
        "      <td> Is there anything Kim Kardashian can't do? In ...</td>\n",
        "      <td>  kardashian</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>22</th>\n",
        "      <td>                                               NaN</td>\n",
        "      <td> The sisters unveil their line of clothing and ...</td>\n",
        "      <td>  kardashian</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>23</th>\n",
        "      <td>                                               NaN</td>\n",
        "      <td> Marriage is coming after the baby carriage for...</td>\n",
        "      <td>  kardashian</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>24</th>\n",
        "      <td> Bahrain's continuing attempt to change the sub...</td>\n",
        "      <td>                                               NaN</td>\n",
        "      <td>  kardashian</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>25</th>\n",
        "      <td>                                               NaN</td>\n",
        "      <td> On \u201cSmart Girls at the Party,\u201d an Austin-based...</td>\n",
        "      <td>  kardashian</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>26</th>\n",
        "      <td>                                               NaN</td>\n",
        "      <td> Kim Kardashian and Kanye West sued a co-founde...</td>\n",
        "      <td>  kardashian</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>27</th>\n",
        "      <td> All the fashion news of the week that's fit to...</td>\n",
        "      <td> All the fashion news of the week that's fit to...</td>\n",
        "      <td>  kardashian</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>28</th>\n",
        "      <td>                                               NaN</td>\n",
        "      <td> Kim Kardashian is pregnant with Kanye West\u2019s b...</td>\n",
        "      <td>  kardashian</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>29</th>\n",
        "      <td> All the fashion news of the week that's fit to...</td>\n",
        "      <td> All the fashion news of the week that's fit to...</td>\n",
        "      <td>  kardashian</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "                                             abstract                                               lead         term\n",
        "0                                                 NaN  A term for mental training reaches the height ...  mindfulness\n",
        "1   Mindfulness is terrific for the person practic...  The other night at a dinner party, a friend de...  mindfulness\n",
        "2                                                 NaN  Mindfulness is terrific for the person practic...  mindfulness\n",
        "3   Jennifer Egan article on her experience at Spi...  Peter Williams sits cross-legged on an upholst...  mindfulness\n",
        "4   Two-thirds of doctors experience the emotional...                                                NaN  mindfulness\n",
        "5   In this week's Doctor and Patient column, Dr. ...  Everybody has to multitask at work. But when d...  mindfulness\n",
        "6                                                 NaN  One night during my training, long after all t...  mindfulness\n",
        "7   Dr Michael S Krasner, University of Rochester ...  One night during my training, long after all t...  mindfulness\n",
        "8   Alina Tugend Shortcuts column observes that mi...  Misconceptions surround the practice of mindfu...  mindfulness\n",
        "9                                                 NaN  In this week&#39;s Doctor and Patient column, ...  mindfulness\n",
        "10  Paul Krugman Op-Ed column contends decision by...  It\u2019s another case of ideology posing as econom...         debt\n",
        "11  Frank Bruni Op-Ed column expresses alarm at th...  So much treasure, so much waste. Italy is a ca...         debt\n",
        "12                                                NaN  In 2008, a year when the global financial syst...         debt\n",
        "13  Paul Krugman Op-Ed column Germany's angry reac...  They are beggaring their neighbors, and the wo...         debt\n",
        "14                                                NaN  ''The Debt,'' John Madden's remake of a 2007 I...         debt\n",
        "15  Jason Anthony and Karl Cluck discuss their boo...  Been there, spent that. It took two Columbia g...         debt\n",
        "16  Some student loan rates doubled as of July 1, ...                                                NaN         debt\n",
        "17                                                NaN  What we have is what we have. We have to rely ...         debt\n",
        "18  Stephen Holden reviews documentary Life and De...  The term ''globalization'' is so tinged with r...         debt\n",
        "19  Paul Krugman Op-Ed column warns policy makers ...  The long-term costs of short-run failure are p...         debt\n",
        "20  Kim Kardashian, sexy reality television star a...  FOR many longtime readers of W, the elite soci...   kardashian\n",
        "21  Kim Kardashian discusses growing up in Beverly...  Is there anything Kim Kardashian can't do? In ...   kardashian\n",
        "22                                                NaN  The sisters unveil their line of clothing and ...   kardashian\n",
        "23                                                NaN  Marriage is coming after the baby carriage for...   kardashian\n",
        "24  Bahrain's continuing attempt to change the sub...                                                NaN   kardashian\n",
        "25                                                NaN  On \u201cSmart Girls at the Party,\u201d an Austin-based...   kardashian\n",
        "26                                                NaN  Kim Kardashian and Kanye West sued a co-founde...   kardashian\n",
        "27  All the fashion news of the week that's fit to...  All the fashion news of the week that's fit to...   kardashian\n",
        "28                                                NaN  Kim Kardashian is pregnant with Kanye West\u2019s b...   kardashian\n",
        "29  All the fashion news of the week that's fit to...  All the fashion news of the week that's fit to...   kardashian"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div class=\"Andrew\">\n",
      "Below is an example of the structure of our list data[]"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## can search for lead paragraph for a particular term \n",
      "## just need to add .values so pandas knows it's not looking for an index\n",
      "\n",
      "print data_df.lead[data_df.term == 'debt'].values[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "It\u2019s another case of ideology posing as economic analysis.\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div class=\"Andrew\">\n",
      "Now we'll pull in sentiment analysis data.  \n",
      "<br />This is the database being used at <a href=\"http://onehappybird.com\">onehappybird.com.\n",
      "</a></div>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## read sentiment data from text file\n",
      "sentiment = pd.read_table('sentiment_data.txt', sep='\\t')\n",
      "\n",
      "## have a look\n",
      "sentiment.head()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>word</th>\n",
        "      <th>happiness_rank</th>\n",
        "      <th>happiness_average</th>\n",
        "      <th>happiness_standard_deviation</th>\n",
        "      <th>twitter_rank</th>\n",
        "      <th>google_rank</th>\n",
        "      <th>nyt_rank</th>\n",
        "      <th>lyrics_rank</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>  laughter</td>\n",
        "      <td> 1</td>\n",
        "      <td> 8.50</td>\n",
        "      <td> 0.9313</td>\n",
        "      <td> 3600</td>\n",
        "      <td>  NaN</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> 1728</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> happiness</td>\n",
        "      <td> 2</td>\n",
        "      <td> 8.44</td>\n",
        "      <td> 0.9723</td>\n",
        "      <td> 1853</td>\n",
        "      <td> 2458</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> 1230</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td>      love</td>\n",
        "      <td> 3</td>\n",
        "      <td> 8.42</td>\n",
        "      <td> 1.1082</td>\n",
        "      <td>   25</td>\n",
        "      <td>  317</td>\n",
        "      <td>  328</td>\n",
        "      <td>   23</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td>     happy</td>\n",
        "      <td> 4</td>\n",
        "      <td> 8.30</td>\n",
        "      <td> 0.9949</td>\n",
        "      <td>   65</td>\n",
        "      <td> 1372</td>\n",
        "      <td> 1313</td>\n",
        "      <td>  375</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td>   laughed</td>\n",
        "      <td> 5</td>\n",
        "      <td> 8.26</td>\n",
        "      <td> 1.1572</td>\n",
        "      <td> 3334</td>\n",
        "      <td> 3542</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> 2332</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "        word  happiness_rank  happiness_average  happiness_standard_deviation  twitter_rank  google_rank  nyt_rank  lyrics_rank\n",
        "0   laughter               1               8.50                        0.9313          3600          NaN       NaN         1728\n",
        "1  happiness               2               8.44                        0.9723          1853         2458       NaN         1230\n",
        "2       love               3               8.42                        1.1082            25          317       328           23\n",
        "3      happy               4               8.30                        0.9949            65         1372      1313          375\n",
        "4    laughed               5               8.26                        1.1572          3334         3542       NaN         2332"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div class=\"Andrew\">\n",
      "Eventually we'll want to run the analysis on all lead paragraphs, across all search terms. \n",
      "<br /><br />\n",
      "For now, we'll narrow it down to paragraphs from just one search term, to keep it simple.\n",
      "</div>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## for now, set 'leads' variable as lead paragraphs for second search term ('debt')\n",
      "leads = list(data_df.lead[data_df.term == 'debt'])\n",
      "print leads\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['It\\xe2\\x80\\x99s another case of ideology posing as economic analysis.', 'So much treasure, so much waste. Italy is a cautionary tale of problems unfixed and pessimism unredeemed.', 'In 2008, a year when the global financial system was falling apart and markets panicked, these investors didn\\xe2\\x80\\x99t. How three big risks, taken by three firms, turned to gold.', 'They are beggaring their neighbors, and the world, by running big trade surpluses.', \"''The Debt,'' John Madden's remake of a 2007 Israeli thriller, shuttles back and forth between a dim, creaky East Berlin apartment in 1965 and the sunshine of elite Tel Aviv a little more than 30 years later. The film, written by Matthew Vaughn, Jane Goldman and Peter Straughan, is interested in the ways that the truth of the past can be shaded and illuminated by the imperatives of the present, and it probes, with perhaps more energy than clarity, the ethical and psychological complications that can lie hidden beneath a story of simple heroism. In this case the official, heroic story has to do with what happened in cold war Berlin, where three Mossad operatives went on a secret mission to capture a Nazi fugitive named Dieter Vogel. Their daring, widely celebrated exploits become the subject of a book written in 1997 by Sarah Gold (Romi Aboulafia), the daughter of two of the agents: Rachel Singer (Helen Mirren) and her ex-husband, Stephan Gold (Tom Wilkinson). Rachel, particularly, has been lionized for the courage she displayed in her younger days. She not only had the nerve to entrap her diabolical quarry, a Mengele-like doctor whose gruesome nickname was the Surgeon of Birkenau, but she also had the presence of mind to shoot him dead when he tried to escape.\", \"Been there, spent that. It took two Columbia graduates who were up to their eyeballs in credit card bills to write ''Debt-Free by 30: Practical Advice for the Young, Broke and Upwardly Mobile,'' just published by Plume ($12). ''The book is for people who think that the fifth floor at Barneys is their birthright,'' said Jason Anthony, who wrote ''Debt Free'' with Karl Cluck. As they see it, the problem is that 20-somethings were brought up to spend lavishly. ''In the 80's, when you saw the rich and fabulous on television, it was 'Dallas,' and they were in their 50's,'' Mr. Anthony said. ''For us, it was 'Beverly Hills 90210,' and people said, 'Well, why can't I have what Tori Spelling has?' ''\", nan, \"What we have is what we have. We have to rely on the money that we're bringing in. LISA MERHAUT, of Leesburg, Va., on her family's effort to cut its credit card debt. [A1]\", \"The term ''globalization'' is so tinged with rosy one-world optimism that it's easy to assume the essential benignity of an economic philosophy whose name vaguely connotes unity, equality and freedom. But as Stephanie Black's powerful documentary ''Life and Debt'' illustrates with an impressive (and depressing) acuity, globalization can have a devastating impact on third world countries. The movie offers the clearest analysis of globalization and its negative effects that I've ever seen on a movie or television screen. ''Life and Debt,'' which opens the Human Rights Watch Film Festival this evening at the Walter Reade Theater and continues its run on Saturday at Cinema Village, focuses on the deeply troubled economy of Jamaica and how that country's long-term indebtedness to international lending organizations have contributed to the erosion of local agriculture and industry.\", 'The long-term costs of short-run failure are piling up and up and up and up.']\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## these three lists are our bins for collecting data on each lead paragraph\n",
      "avg_word = []\n",
      "post_length = []\n",
      "avg_sent = []\n",
      "avg_hard = []\n",
      "avg_dyn = []\n",
      "wordbag = set()\n",
      "\n",
      "\n",
      "\n",
      "## loop through lead paragraphs\n",
      "for item in leads:\n",
      "    \n",
      "    ## outer_sent_hard collects words with sentiment ratings more extreme than a hard-coded dispersion from the mean\n",
      "    ## eg. if mean is 5.2 and we set hard dispersion to be 1.0, then outer_sent_hard collects words with sentiment\n",
      "    ##     at <4.2 and >6.2.  \n",
      "    outer_sent_hard = set()\n",
      "    \n",
      "    ## outer_sent_dyn collects words with sentiment ratings more extreme than a dynamically assessed dispersion\n",
      "    ## eg. if range is 2.0 to 8.0 and dynamic dispersion is set at 10% of total range\n",
      "    ##     then outer_sent_dyn collects words with sentiment more extreme than .1(6.0) from mean sentiment in corpus.\n",
      "    ##     ie. with mean 5.2, collect words with sentiment <4.6 and >5.8\n",
      "    outer_sent_dyn = set()\n",
      "    \n",
      "    \n",
      "    ## AR (nov 25):\n",
      "    ## this is where we might want to exclude bodies of text with fewer than 1000 words\n",
      "    ## (according to our sentiment analysis guy, <1000 words is when the analytic power starts to drop off)\n",
      "    ## \n",
      "    ## i'm not sure that will really fly with the nyt api though, as we only get lead paragraphs.\n",
      "    if (isinstance(item, float)):\n",
      "        continue\n",
      "    words_df = pd.DataFrame(item.split(' '), columns=['words'])\n",
      "    words_df['words'] = words_df.words.apply(lambda x: re.sub(\"\\W\", '', x))\n",
      "    ## gets bag of words for the entire corpus\n",
      "    wordbag |= set(words_df['words'].values)\n",
      "    words_df['word_length'] = words_df.words.apply(lambda x: len(x))\n",
      "    \n",
      "    words_df = words_df[words_df.word_length > 0]\n",
      "    \n",
      "    wdct = len(words_df)\n",
      "    word_lengths = list(words_df.word_length.values)\n",
      "    \n",
      "    words_df['in_sentiment'] = False\n",
      "    words_df['in_sentiment'][words_df.words.isin(sentiment.word)] = True\n",
      "    \n",
      "    # 'happiness average' is the same as 'sentiment score'\n",
      "    words_df['happiness_avg'] = np.nan\n",
      "    words_df['happiness_avg'][words_df.in_sentiment == True] = words_df.words[words_df.in_sentiment == True].apply(lambda x: \n",
      "            float(sentiment.happiness_average[sentiment.word == x].values))\n",
      "                                                     \n",
      "    # NOTE: the average word length here is including words not in the sentiment df\n",
      "    # maybe these words shouldn't be included in this calculation?\n",
      "    \n",
      "    ## AR (Nov 25): I think it's okay, as we're not attempting to draw a direct connection between the length\n",
      "    ##                  of each individual word and its sentiment, but rather the overall length of words in the post\n",
      "    ##                  and the overall sentiment that we can assess.  Even so, we can run it both ways and see if\n",
      "    ##                  it makes a big difference.\n",
      "    avg_word_length = round(np.mean(words_df.word_length.values), 2)\n",
      "    avg_sent_score = round(np.mean(words_df.happiness_avg[np.isfinite(words_df.happiness_avg)].values), 3)\n",
      "    \n",
      "    ## these vars are for assessing dynamic evaluation of our excluded middle vlaues\n",
      "    ##\n",
      "    ## sent_min: lowest sentiment score in this corpus\n",
      "    sent_min = words_df['happiness_avg'].min(axis=0)\n",
      "    \n",
      "    print 'sent_min:',sent_min\n",
      "    ## sent_max: highest sentiment score in this corpus\n",
      "    sent_max = words_df['happiness_avg'].max(axis=0)\n",
      "    \n",
      "    print 'sent_max:',sent_max\n",
      "    ## sent_range: difference between max and min\n",
      "    sent_range = sent_max - sent_min\n",
      "    \n",
      "    print 'sent_range:',sent_range\n",
      "    ## range_constant: multiply sent_range by this number to get the dispersion from the mean we want to exclude\n",
      "    ##                (this could be done using stdev, or by hard-coding a percentage, as I've done here)\n",
      "    ## \n",
      "    ## for now, use 20%.\n",
      "    range_constant = 0.2\n",
      "    dispersion = range_constant * sent_range\n",
      "    print 'dispersion constant:', dispersion, 'away from mean:',avg_sent_score\n",
      "    \n",
      "    ## lower/upper boundaries for hard-coded exclusion\n",
      "    hard_exclude_min = avg_sent_score - 1\n",
      "    hard_exclude_max = avg_sent_score + 1\n",
      "    \n",
      "    ## lower and upper boundaries for dynamic exclusion\n",
      "    dyn_exclude_min = avg_sent_score - dispersion\n",
      "    dyn_exclude_max = avg_sent_score + dispersion\n",
      "    \n",
      "    print 'exclude values between',exclude_min,'and',exclude_max\n",
      "    print '\\n\\n' \n",
      "    ## go through all the words in a corpus\n",
      "    ## filter by either hard or dynamic dispersion from mean\n",
      "    ## store the words we want in new arrays\n",
      "    for row in words_df.iterrows():\n",
      "        ## print row[1]\n",
      "        datum = row[1]\n",
      "        \n",
      "        ##\n",
      "        ##\n",
      "        ## what is the nan problem?\n",
      "        ##\n",
      "        ##\n",
      "        \n",
      "        if ((datum['happiness_avg'] > hard_exclude_max) or (datum['happiness_avg'] < hard_exclude_min)):\n",
      "            \n",
      "            outer_sent_hard.add(datum['happiness_avg'])\n",
      "            \n",
      "        if ((datum['happiness_avg'] > dyn_exclude_max) or (datum['happiness_avg'] < dyn_exclude_min)):\n",
      "            \n",
      "            outer_sent_dyn.add(datum['happiness_avg'])\n",
      "    \n",
      "    ## print 'outer_sent_hard:',outer_sent_hard\n",
      "    avg_hard_score = round(np.mean(list(outer_sent_hard)),3)\n",
      "    avg_dyn_score = round(np.mean(list(outer_sent_dyn)),3)\n",
      "    \n",
      "    ## post length is the word count\n",
      "    post_length.append(wdct)    \n",
      "    \n",
      "    ## append averages of word lengths and sentiment scores to bins\n",
      "    avg_word.append(avg_word_length)\n",
      "    avg_sent.append(avg_sent_score)\n",
      "    avg_dyn.append(avg_dyn_score)\n",
      " \n",
      "    ## in some rare instances, max dispersion in actual data is less than hard-coded range, so literally no values are captured\n",
      "    ## however excluding those sets from the master list causes problems later when we want a dataframe\n",
      "    ## so keep them in as NaN for now and exclude later.\n",
      "    avg_hard.append(avg_hard_score)\n",
      "    \n",
      "print avg_word\n",
      "print avg_sent\n",
      "print avg_hard\n",
      "print avg_dyn\n",
      "print post_length\n",
      "print '\\n\\n'\n",
      "\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "sent_min: 4.94\n",
        "sent_max: 5.82\n",
        "sent_range: 0.88\n",
        "dispersion constant: 0.176 away from mean: 5.337\n",
        "exclude values between 4.291 and 5.923\n",
        "\n",
        "\n",
        "\n",
        "sent_min: 2.7\n",
        "sent_max: 7.4\n",
        "sent_range: 4.7\n",
        "dispersion constant: 0.94 away from mean: 5.127\n",
        "exclude values between 4.291 and 5.923\n",
        "\n",
        "\n",
        "\n",
        "sent_min: 3.28\n",
        "sent_max: 7.56\n",
        "sent_range: 4.28\n",
        "dispersion constant: 0.856 away from mean: 5.129\n",
        "exclude values between 4.291 and 5.923\n",
        "\n",
        "\n",
        "\n",
        "sent_min:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4.98\n",
        "sent_max: 6.52\n",
        "sent_range: 1.54\n",
        "dispersion constant: 0.308 away from mean: 5.602\n",
        "exclude values between 4.291 and 5.923\n",
        "\n",
        "\n",
        "\n",
        "sent_min:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.8\n",
        "sent_max: 7.94\n",
        "sent_range: 6.14\n",
        "dispersion constant: 1.228 away from mean: 5.278\n",
        "exclude values between 4.291 and 5.923\n",
        "\n",
        "\n",
        "\n",
        "sent_min:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2.82\n",
        "sent_max: 7.98\n",
        "sent_range: 5.16\n",
        "dispersion constant: 1.032 away from mean: 5.332\n",
        "exclude values between 4.291 and 5.923\n",
        "\n",
        "\n",
        "\n",
        "sent_min: 2.9\n",
        "sent_max: 7.3\n",
        "sent_range: 4.4\n",
        "dispersion constant: 0.88 away from mean: 5.382\n",
        "exclude values between 4.291 and 5.923\n",
        "\n",
        "\n",
        "\n",
        "sent_min:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.9\n",
        "sent_max: 7.9\n",
        "sent_range: 6.0\n",
        "dispersion constant: 1.2 away from mean: 5.413\n",
        "exclude values between 4.291 and 5.923\n",
        "\n",
        "\n",
        "\n",
        "sent_min: 2.06\n",
        "sent_max: 6.14\n",
        "sent_range: 4.08\n",
        "dispersion constant: 0.816 away from mean: 5.107\n",
        "exclude values between 4.291 and 5.923\n",
        "\n",
        "\n",
        "\n",
        "[5.33, 5.06, 4.69, 5.15, 4.7, 4.29, 3.74, 5.47, 3.93]\n",
        "[5.337, 5.127, 5.129, 5.602, 5.278, 5.332, 5.382, 5.413, 5.107]\n",
        "[nan, 4.815, 4.727, nan, 5.421, 5.506, 4.54, 5.859, 4.0]\n",
        "[5.395, 4.815, 5.058, 5.713, 5.515, 5.506, 5.0, 5.63, 4.0]\n",
        "[9, 17, 29, 13, 218, 122, 34, 133, 15]\n",
        "\n",
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Avg sentiment (no exclusion):',np.mean(avg_sent)\n",
      "print 'Avg sentiment (hard-code 1.0 spread exclusion):', stats.nanmean(avg_hard)\n",
      "print 'Avg sentiment (dynamic 20% exclusion):', np.mean(avg_dyn)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Avg sentiment (no exclusion): 5.30077777778\n",
        "Avg sentiment (hard-code 1.0 spread exclusion): 4.98114285714\n",
        "Avg sentiment (dynamic 20% exclusion): 5.18133333333\n"
       ]
      }
     ],
     "prompt_number": 49
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div class=\"Andrew\">\n",
      "Pandas dataframes make it easy to collect and reference the vectors we're creating.\n",
      "</div>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## make a pandas df out of these vectors for easy reference/manipulation\n",
      "post_df = pd.DataFrame(data={'avg word length':avg_word, \n",
      "                             'post length':post_length, \n",
      "                             'avg_sent_all':avg_sent,\n",
      "                             'avg_sent_hard':avg_hard,\n",
      "                             'avg_sent_dyn':avg_dyn})\n",
      "\n",
      "## some posts may get entered when technically they're not posts...just 0-length strings.  so exclude those.\n",
      "## we can probably get rid of these earlier on...\n",
      "trimmed_post_df = post_df[post_df['post length'] > 1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 47
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div class=\"Andrew\">\n",
      "We can do simple Pearson's correlation statistics to see whether there's any relationship between our variables of interest.\n",
      "</div>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## pearsonr() gives a tuple of (Pearson zero-order correlation, p-value)\n",
      "\n",
      "## here we print out the correlation matrix for avg word length, post length, and avg sentiment\n",
      "## (not very pretty though - better to output in a readable table, a la SPSS)\n",
      "\n",
      "print 'avg word length : post length'\n",
      "print stats.pearsonr(trimmed_post_df['avg word length'].values, trimmed_post_df['post length'].values)\n",
      "\n",
      "print '\\n\\n'\n",
      "\n",
      "print 'avg word length : avg sentiment'\n",
      "print stats.pearsonr(trimmed_post_df['avg word length'].values, trimmed_post_df['avg_sent_all'].values)\n",
      "\n",
      "print 'post length : avg sentiment'\n",
      "print stats.pearsonr(trimmed_post_df['post length'].values, trimmed_post_df['avg_sent_all'].values)\n",
      "\n",
      "print '\\n\\n'\n",
      "\n",
      "hard = trimmed_post_df.copy()\n",
      "hard = hard.dropna()\n",
      "\n",
      "print 'avg word length : avg sentiment hard-coded'\n",
      "print stats.pearsonr(hard['avg word length'].values, hard['avg_sent_hard'].values)\n",
      "\n",
      "print 'post length : avg sentiment hard-coded'\n",
      "print stats.pearsonr(hard['post length'].values, hard['avg_sent_hard'].values)\n",
      "\n",
      "print '\\n\\n'\n",
      "\n",
      "print 'avg word length : avg sentiment dynamically-coded'\n",
      "print stats.pearsonr(trimmed_post_df['avg word length'].values, trimmed_post_df['avg_sent_dyn'].values)\n",
      "\n",
      "print 'post length : avg sentiment dynamically-coded'\n",
      "print stats.pearsonr(trimmed_post_df['post length'].values, trimmed_post_df['avg_sent_dyn'].values)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "avg word length : post length\n",
        "(0.059334919406730677, 0.87947796679588963)\n",
        "\n",
        "\n",
        "\n",
        "avg word length : avg sentiment\n",
        "(0.30515721016965741, 0.4245813116386381)\n",
        "post length : avg sentiment\n",
        "(0.10309637162286636, 0.79182519284738651)\n",
        "\n",
        "\n",
        "\n",
        "avg word length : avg sentiment hard-coded\n",
        "(0.66772144479423401, 0.10119940373258697)\n",
        "post length : avg sentiment hard-coded\n",
        "(0.78531888939615635, 0.036397543524932463)\n",
        "\n",
        "\n",
        "\n",
        "avg word length : avg sentiment dynamically-coded\n",
        "(0.58252653083424821, 0.099767885077456694)\n",
        "post length : avg sentiment dynamically-coded\n",
        "(0.45925831244841014, 0.21364378722266858)\n"
       ]
      }
     ],
     "prompt_number": 50
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div class=\"Andrew\">It looks like word length and sentiment is trending toward a significant positive correlation, and possibly post length and avg sentiment heading towards a negative.  \n",
      "<br />\n",
      "    It's a really small sample compared to our entire corpus, so best to retain some skepticism...next is to run on a dozen or so major search terms and look at the aggregate correlations.</div>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##\n",
      "## regression attempt using sklearn\n",
      "##\n",
      "\n",
      "print hard['avg word length'].values\n",
      "clf = linear_model.LinearRegression()\n",
      "clf.fit([hard['avg word length'].values, hard['post length'].values], hard['avg_sent_hard'].values)\n",
      "print clf.coef_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ValueError",
       "evalue": "incompatible dimensions",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-60-06e0ea44002e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mhard\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'avg word length'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhard\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'avg word length'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhard\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'post length'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhard\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'avg_sent_hard'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/DharmaLoka/anaconda/lib/python2.7/site-packages/sklearn/linear_model/base.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, n_jobs)\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresidues_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingular_\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                 \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstsq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/DharmaLoka/anaconda/lib/python2.7/site-packages/scipy/linalg/basic.pyc\u001b[0m in \u001b[0;36mlstsq\u001b[0;34m(a, b, cond, overwrite_a, overwrite_b, check_finite)\u001b[0m\n\u001b[1;32m    512\u001b[0m         \u001b[0mnrhs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'incompatible dimensions'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m     \u001b[0mgelss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_lapack_funcs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gelss'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mValueError\u001b[0m: incompatible dimensions"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 5.06  4.69  4.7   4.29  3.74  5.47  3.93]\n"
       ]
      }
     ],
     "prompt_number": 60
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Everything below is pasted from HW3...it's where we want to go next.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text = leads\n",
      "print \"Original text is\\n\", '\\n'.join(text)\n",
      "\n",
      "vectorizer = CountVectorizer(min_df=0)\n",
      "\n",
      "# call `fit` to build the vocabulary\n",
      "vectorizer.fit(text)\n",
      "\n",
      "# call `transform` to convert text to a bag of words\n",
      "x = vectorizer.transform(text)\n",
      "\n",
      "# CountVectorizer uses a sparse array to save memory, but it's easier in this assignment to \n",
      "# convert back to a \"normal\" numpy array\n",
      "x = x.tocsc()\n",
      "\n",
      "#print\n",
      "#print \"Transformed text vector is \\n\", x\n",
      "\n",
      "# `get_feature_names` tracks which word is associated with each column of the transformed x\n",
      "print\n",
      "print \"Words for each feature:\"\n",
      "print vectorizer.get_feature_names()\n",
      "\n",
      "# Notice that the bag of words treatment doesn't preserve information about the *order* of words, \n",
      "# just their frequency"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Original text is\n",
        "The sky is falling! The sky is falling. When it comes to all the talk of economic catastrophe, why are these disaster fantasies taken so seriously?\n",
        "It\u2019s another case of ideology posing as economic analysis.\n",
        "The International Paper Company, manufacturing subsidiary of the International Paper and Power Company, reported yesterday a net loss of $2,413,866 for 1934, after $595.849 profit from bonds redeemed in the year. This compared with a net loss of $4,430,528, after profit of $913,937 from bonds redeemed, in 1933.\n",
        "Will inflation phobia tear Europe apart after the establishment of a common currency was supposed to unite the Continent?\n",
        "After being sentenced in April 1982 to 15 years to life in prison for killing a waiter in Manhattan, Jack Henry Abbott was found liable for monetary damages sought by the victim's widow. A State Supreme Court justice ordered a civil trial to determine how much money the widow, Ricci Adan, should get.\n",
        "few years threatened the developing countries and the world financial system, suddenly over? Or is it just taking a holiday? William R. Rhodes of Citicorp, who has been head of the committee of banks negotiating with the Latin American countries over the rescheduling of their debts, warns against complacency that ''the debt bomb has been defused.'' ''That characterization is too strong,'' he said at a conference on the third world at Duke University this week. ''The reality is that a great deal remains to be done before these countries get back on the road to the growth they have enjoyed in the past.''\n",
        "So much treasure, so much waste. Italy is a cautionary tale of problems unfixed and pessimism unredeemed.\n",
        "In 2008, a year when the global financial system was falling apart and markets panicked, these investors didn\u2019t. How three big risks, taken by three firms, turned to gold.\n",
        "\n",
        "Words for each feature:\n",
        "[u'15', u'1933', u'1934', u'1982', u'2008', u'413', u'430', u'528', u'595', u'849', u'866', u'913', u'937', u'abbott', u'adan', u'after', u'against', u'all', u'american', u'analysis', u'and', u'another', u'apart', u'april', u'are', u'as', u'at', u'back', u'banks', u'be', u'been', u'before', u'being', u'big', u'bomb', u'bonds', u'by', u'case', u'catastrophe', u'cautionary', u'characterization', u'citicorp', u'civil', u'comes', u'committee', u'common', u'company', u'compared', u'complacency', u'conference', u'continent', u'countries', u'court', u'currency', u'damages', u'deal', u'debt', u'debts', u'defused', u'determine', u'developing', u'didn', u'disaster', u'done', u'duke', u'economic', u'enjoyed', u'establishment', u'europe', u'falling', u'fantasies', u'few', u'financial', u'firms', u'for', u'found', u'from', u'get', u'global', u'gold', u'great', u'growth', u'has', u'have', u'he', u'head', u'henry', u'holiday', u'how', u'ideology', u'in', u'inflation', u'international', u'investors', u'is', u'it', u'italy', u'jack', u'just', u'justice', u'killing', u'latin', u'liable', u'life', u'loss', u'manhattan', u'manufacturing', u'markets', u'monetary', u'money', u'much', u'negotiating', u'net', u'of', u'on', u'or', u'ordered', u'over', u'panicked', u'paper', u'past', u'pessimism', u'phobia', u'posing', u'power', u'prison', u'problems', u'profit', u'reality', u'redeemed', u'remains', u'reported', u'rescheduling', u'rhodes', u'ricci', u'risks', u'road', u'said', u'sentenced', u'seriously', u'should', u'sky', u'so', u'sought', u'state', u'strong', u'subsidiary', u'suddenly', u'supposed', u'supreme', u'system', u'taken', u'taking', u'tale', u'talk', u'tear', u'that', u'the', u'their', u'these', u'they', u'third', u'this', u'threatened', u'three', u'to', u'too', u'treasure', u'trial', u'turned', u'unfixed', u'unite', u'university', u'unredeemed', u'victim', u'waiter', u'warns', u'was', u'waste', u'week', u'when', u'who', u'why', u'widow', u'will', u'william', u'with', u'world', u'year', u'years', u'yesterday']\n"
       ]
      }
     ],
     "prompt_number": 89
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def make_xy(pa, vectorizer=None):\n",
      "    #Your code here    \n",
      "\n",
      "    X = []\n",
      "    Y = []\n",
      "    \n",
      "    # if no vectorizer is passed to function, create one with min_df=0\n",
      "    if vectorizer == None:\n",
      "        \n",
      "        vectorizer = CountVectorizer(min_df=0)\n",
      "    \n",
      "    # fit vectorizer to entire quote repertoire\n",
      "    vectorizer.fit(critics.quote)\n",
      "    # make bag of words from quotes\n",
      "    X = vectorizer.transform(critics.quote)\n",
      "    \n",
      "    # convert fresh/rotten (strings) to 1/0 ints\n",
      "    for idx, q in enumerate(critics['quote']):\n",
      "        \n",
      "        fresh = critics['fresh'].irow(idx)\n",
      "        if fresh == 'fresh':\n",
      "            val = 1\n",
      "        elif fresh == 'rotten':\n",
      "            val = 0\n",
      "        Y.append(val)\n",
      "    \n",
      "    # return X/Y\n",
      "    # X can stay as a scarce array for now, Y covert from list to numpy array\n",
      "    return X,np.asarray(Y)\n",
      "\n",
      "X, Y = make_xy(critics)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 91,
       "text": [
        "\"def make_xy(pa, vectorizer=None):\\n    #Your code here    \\n\\n    X = []\\n    Y = []\\n    \\n    # if no vectorizer is passed to function, create one with min_df=0\\n    if vectorizer == None:\\n        \\n        vectorizer = CountVectorizer(min_df=0)\\n    \\n    # fit vectorizer to entire quote repertoire\\n    vectorizer.fit(critics.quote)\\n    # make bag of words from quotes\\n    X = vectorizer.transform(critics.quote)\\n    \\n    # convert fresh/rotten (strings) to 1/0 ints\\n    for idx, q in enumerate(critics['quote']):\\n        \\n        fresh = critics['fresh'].irow(idx)\\n        if fresh == 'fresh':\\n            val = 1\\n        elif fresh == 'rotten':\\n            val = 0\\n        Y.append(val)\\n    \\n    # return X/Y\\n    # X can stay as a scarce array for now, Y covert from list to numpy array\\n    return X,np.asarray(Y)\\n\\nX, Y = make_xy(critics)\\n\""
       ]
      }
     ],
     "prompt_number": 91
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}